{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89ddcaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee9a5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeTensor(torch.Tensor):\n",
    "    def __new__(cls, data: torch.Tensor, shift: float = None, eps: float = 1e-8):\n",
    "        # Создаем объект как подкласс\n",
    "        obj = torch.Tensor._make_subclass(cls, data.clone().detach())\n",
    "        \n",
    "        # Вычисляем параметры сдвига\n",
    "        abs_data = data.abs()\n",
    "        min_val = data.min().item()\n",
    "        min_positive = abs_data[abs_data > 0].min().item() if (abs_data > 0).any() else 1.0\n",
    "        computed_shift = abs(min_val) + min_positive\n",
    "\n",
    "        # Инициализируем атрибуты объекта\n",
    "        obj._eps = eps\n",
    "        obj._shift = shift if shift is not None else computed_shift\n",
    "        obj._logdata = torch.log2(data.abs() + obj._shift + eps)\n",
    "        \n",
    "        return obj\n",
    "\n",
    "    def _inverse_transform(self):\n",
    "        return torch.exp2(self._logdata) - self._shift\n",
    "\n",
    "    def data(self):\n",
    "        return self._inverse_transform()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"SafeTensor({self.data().__repr__()}, shift={self._shift:.4f})\"\n",
    "\n",
    "    def clone(self):\n",
    "        return SafeTensor(self.data(), self._shift, self._eps)\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        return SafeTensor(self.data().to(*args, **kwargs), self._shift, self._eps)\n",
    "\n",
    "    @property\n",
    "    def log_repr(self):\n",
    "        return self._logdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5cf1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeLinearFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_log, weight_log, bias_log, shift, eps):\n",
    "        # Восстанавливаем оригинальные значения\n",
    "        input_real = torch.exp2(input_log) - shift\n",
    "        weight_real = torch.exp2(weight_log)\n",
    "        bias_real = torch.exp2(bias_log)\n",
    "        \n",
    "        # Линейная операция\n",
    "        output_real = F.linear(input_real, weight_real, bias_real)\n",
    "        \n",
    "        # Возвращаем в лог-пространстве\n",
    "        output_log = torch.log2(output_real.abs() + shift + eps)\n",
    "        return output_log\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_log):\n",
    "        # Здесь должна быть реализация backward, но для простоты пока вернем None\n",
    "        return None, None, None, None, None\n",
    "\n",
    "class SafeLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, shift=1.0, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.shift = shift\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Инициализация параметров в логарифмическом пространстве\n",
    "        weight_real = torch.randn(out_features, in_features) * 0.01\n",
    "        bias_real = torch.zeros(out_features)\n",
    "        \n",
    "        self.weight_log = nn.Parameter(torch.log2(weight_real.abs() + eps))\n",
    "        self.bias_log = nn.Parameter(torch.log2(bias_real.abs() + eps))\n",
    "    \n",
    "    def forward(self, input_log):\n",
    "        return SafeLinearFunction.apply(\n",
    "            input_log, \n",
    "            self.weight_log,\n",
    "            self.bias_log,\n",
    "            self.shift,\n",
    "            self.eps\n",
    "        )\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, shift={self.shift:.4f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f4c1a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = SafeLinear(input_dim, hidden_dim)\n",
    "        self.fc2 = SafeLinear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: SafeTensor):\n",
    "        x = F.relu(self.fc1(x).data())  # real tensor to relu\n",
    "        return self.fc2(SafeTensor(x)).data()  # финальный real output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ee1891cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=10, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            # Прямой проход\n",
    "            optimizer.zero_grad()\n",
    "            safe_inputs = SafeTensor(inputs)\n",
    "            outputs = model(safe_inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Обратный проход\n",
    "            loss.backward()\n",
    "            # Шаг оптимизации\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Эпоха {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "def get_synthetic_data(n_samples=1024, input_dim=784, num_classes=10):\n",
    "    X = torch.randn(n_samples, input_dim)\n",
    "    y = torch.randint(0, num_classes, (n_samples,))\n",
    "    return TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09323a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SafeTensor' object has no attribute '_logdata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m SafeMLP(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Обучение с защитой от битфлипов\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, epochs, lr)\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m safe_inputs \u001b[38;5;241m=\u001b[39m SafeTensor(inputs)\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msafe_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Обратный проход\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[76], line 8\u001b[0m, in \u001b[0;36mSafeMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: SafeTensor):\n\u001b[0;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# real tensor to relu\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(SafeTensor(x))\u001b[38;5;241m.\u001b[39mdata()\n",
      "Cell \u001b[0;32mIn[74], line 23\u001b[0m, in \u001b[0;36mSafeTensor.data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdata\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 20\u001b[0m, in \u001b[0;36mSafeTensor._inverse_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_inverse_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mexp2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logdata\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shift\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SafeTensor' object has no attribute '_logdata'"
     ]
    }
   ],
   "source": [
    "dataset = get_synthetic_data()\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "model = SafeMLP(input_dim=784, hidden_dim=128, output_dim=10)\n",
    "# Обучение с защитой от битфлипов\n",
    "train(model, dataloader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74958ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
